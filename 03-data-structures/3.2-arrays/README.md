# B"H


## Arrays

Arrays are a special data structure in Go that allow us to allocate contiguous blocks of fixed size memory. Arrays have some special features in Go related to how they are declared and viewed as types.

## Notes

* If you don't understand the data, you don't understand the problem.
* If you don't understand the cost of solving the problem, you can't reason about the problem.
* If you don't understand the hardware, you can't reason about the cost of solving the problem.
* Arrays are fixed length data structures that can't change.
* Arrays of different sizes are considered to be of different types.
* Memory is allocated as a contiguous block.
* Go gives you control over spacial locality.



## CPU Caches

[CPU Caches and Why You Care (18:50-20:30)](https://youtu.be/WDIkqP4JbkE?t=1129) - Scott Meyers  
[CPU Caches and Why You Care (44:36-45:40)](https://youtu.be/WDIkqP4JbkE?t=2676) - Scott Meyers   
[Performance Through Cache-Friendliness (4:25-5:48)](https://youtu.be/jEG4Qyo_4Bc?t=266) - Damian Gryski  

## CPU Cache Notes

* CPU caches works by caching main memory on cache lines.
* Cache lines today are either 32 or 64 bytes wide depending on the hardware.
* Cores do not access main memory directly. They tend to only have access their local caches.
* Both data and instructions are stored in the caches.
* Cache lines are shuffled down L1->L2->L3 as new cache lines need to be stored in the caches.
* Hardware likes to traverse data and instructions linearly along cache lines.
* Main memory is built on relatively fast cheap memory. Caches are built on very fast expensive memory.

* Access to main memory is incredibly slow, we need the cache.
	* Accessing one byte from main memory will cause an entire cache line to be read and cached.
	* Writes to one byte in a cache line requires the entire cache line to be written.

* Small = Fast
	* Compact, well localized code that fits in cache is fastest.
	* Compact data structures that fit in cache are fastest.
	* Traversals touching only cached data is the fastest.

* Predictable access patterns matter.
	* Whenever it is practical, you want to employ a linear array traversal.
	* Provide regular patterns of memory access.
	* Hardware can make better predictions about required memory.

* Cache misses can result in TLB cache misses as well.
    * A translation lookaside buffer (TLB) is a memory cache that is used to reduce the time taken to access a user memory location
	* Cache of translations of a virtual address to a physical address.
	* Waiting on the OS to tell us where the memory is.

### Cache Hierarchies

This is a diagram showing the relationship of the cache hierarchy for the 4 Core i7-9xx processor. The caches in the diagram are not to scale. This processor has four cores and each core has two hardware threads. The hardware threads per core share the Level 1 caches. The cores have individual Level 1 and Level 2 caches. All cores for all the processor share the L3 cache.

![figure1](img/figure1.png)

This is subject to be different in different processors. For this content, the following is the multi-levels of cache associated with the Intel 4 Core i7-9xx processor:

### Intel i7 CPU Latencies From Video

```
3GHz(3 clock cycles/ns) * 4 instructions per cycle = 12 instructions per ns!

1 ns ............. 1 ns .............. 12 instructions  (one) 
1 µs .......... 1000 ns .......... 12,000 instructions  (thousand)
1 ms ..... 1,000,000 ns ...... 12,000,000 instructions  (million)
1 s .. 1,000,000,000 ns .. 12,000,000,000 instructions  (billion)

L1 - 64KB Cache (Per Core)
	4 cycles of latency at 1.3 ns
	Stalls for 16 instructions

L2 - 256KB Cache (Per Core)
	12 cycles of latency at 4 ns
	Stalls for 48 instructions

L3 - 8MB Cache
	40 cycles of latency at 13.3 ns
	Stalls for 160 instructions

Main Memory
	100 cycle of latency at 33.3 ns
	Stalled for 400 instructions
```

### Industry Defined Latencies

```
L1 cache reference ......................... 0.5 ns ...................  6 ins
Branch mispredict ............................ 5 ns ................... 60 ins
L2 cache reference ........................... 7 ns ................... 84 ins
Mutex lock/unlock ........................... 25 ns .................. 300 ins
Main memory reference ...................... 100 ns ................. 1200 ins           
Compress 1K bytes with Zippy ............. 3,000 ns (3 µs) ........... 36k ins
Send 2K bytes over 1 Gbps network ....... 20,000 ns (20 µs) ........  240k ins
SSD random read ........................ 150,000 ns (150 µs) ........ 1.8M ins
Read 1 MB sequentially from memory ..... 250,000 ns (250 µs) .......... 3M ins
Round trip within same datacenter ...... 500,000 ns (0.5 ms) .......... 6M ins
Read 1 MB sequentially from SSD* ..... 1,000,000 ns (1 ms) ........... 12M ins
Disk seek ........................... 10,000,000 ns (10 ms) ......... 120M ins
Read 1 MB sequentially from disk .... 20,000,000 ns (20 ms) ......... 240M ins
Send packet CA->Netherlands->CA .... 150,000,000 ns (150 ms) ........ 1.8B ins
```

---

[Time Calc](http://www.sengpielaudio.com/calculator-millisecond.htm)

![](img/woah.png)

![Cache Latencies Image](img/cache_latencies_graph.png)

#### CPU Caches / Memory


[A Crash Course in Modern Hardware - Video](https://www.youtube.com/watch?v=OFgxAFdxYAQ) - Cliff Click  
 
---

### Deep Dive

Hi, so what we're now gonna start learning is Go's data structures, and we're here in the array section, but before we get there, a little story, when I first came to Go from C#, and I had done C++ before that, one of the things that struck me as odd was all my data structures were gone, where are my lists, and my queues, and my stacks, in these data structures, that for 20 years, I'd been working with, it's gone, all Go gives you is the array, gives you this slice which looks like an array, and it gives you maps, which I can understand, key value pairs, and my brain is just starting to shut down, because when I went to school in the late 80s, we were taught arrays, but we were also taught to stay away from arrays, they were complicated and hard to work with, they were static in size, they were not dynamic, we learned how to use link lists, and other data structures, and I've been using them my whole career, and now I come into Go, and they're all gone. Arrays, slices, and maps, that's all I got. So, when I first started coding in Go, I started just using the link lists package, it's what I knew, it's how I knew how to deal with data, and I started using it, but over and over again, I kept seeing this use of slices, slices were everywhere in the language, and then I started to learn how deeply built in to the standard library they were as well, and the built in functions, and this was a core data structure in Go, and in fact today, the slice is the most important data structure we have in Go. But I didn't understand why, and I'm one of these people where I really wanna understand what I'm doing and why I'm doing it, again, the mechanics, and the semantics, and I can make better engineering choices. So, what I wanna do is share with you a little program that's gonna help us on our journey to understand why Go only has arrays, slices, and maps, and why we should be using these core data structures as our first choice, and when we go beyond that, it's usually more of an exception and not the rule. So let's take a look at this piece of code here. This code starts out defining two constants, rows and columns, both at 2000. And we're gonna use that to create this matrix of bytes. Basically, we're gonna have about four million elements in our matrix, each a byte in size, and what we're gonna end up with is this data structure here, right, nothing fancy, just a big square matrix, it's gonna have rows, it's gonna have columns, and each one of these elements is one byte in size. Then, on line 21, what I've done, is I've defined a data structure which will be simulating my link list, and a link list is nothing more than, you allocate a small block of memory, you put your data into it, and we use a pointer that shares us to the next piece of data somewhere in memory, these were really great data structures that we learned, it's a core data structure that's used by lots of algorithms, because instead of trying to allocate one big contiguous block of memory, we can allocate these smaller ones, and back in the old days, when we didn't have a lot of memory, this type of data structure was very, very important. So, we've got V, our byte, we've got a pointer to the next piece of data, right, it looks recursive, that it can point to itself, and then on line 27, we have the link list pointer, that's the head pointer, that gives us where the beginning of the list begins. Now, in the init function, I'm doing a couple things. I'm gonna, what I want is a node in the linked list for every element, basically, I'm gonna have four million elements in the array, I want four million nodes in my link list, and another thing that this code does, is every other element, and every other node, we reset the byte from zero value to FF, cause we're gonna operate on every other element, or every other node. Here's the real idea behind this code. I wanted to know, what was the CPU performance, how much time did it take, to traverse from beginning to end, the matrix, and the link list. Now the link list is unidirectional, so I want to know, as I traverse, and walk down this data, how fast can I do that, and just to do some extra work, I increment a local counter on every other access. Now, when we talk about traversing a matrix, we can go in two different direction. I can do column major order, where we're going down, down, down, or I can do row major order, where I'm doing across, across, across, across. So it's the same block of memory, but we can traverse it in two different direction. I want you to notice something about the three algorithms, they all are basically four lines of code. I need four lines of code to traverse the link list and increment the local counter, I need four lines of code to do row major traversal, and column major traversal. From an algorithm perspective, these algorithms all have essentially the same kind of algorithm efficiency when it comes to lines of code. I can not reduce the number of lines of code, and I really can't do anything more or less than I'm doing in these functions to make this code any faster, I'm at my highest levels of efficiency right now. Okay, two data structures, three different traversals, two different directions on the matrix, one direction on the link list. But how can we look at how long it takes to do these traversals? Well, Go's testing tool gives us the ability to write benchmarks, and what we're gonna do, is create an underscore test file, you can see here in the name, caching_test, and we're gonna write three functions that start with the word Benchmark, and take a testing B pointer, link list traversal, column traversal, and row traversal. Now, this is what's cool, the way the benchmark works is, the Go testing tool, find these benchmark functions, and call them one at a time, and when it calls the benchmark function, it will set this b.N variable to be one. All of the code that we want to benchmark must be inside this loop. Now it's important that the code inside the loop is as accurate to the way it's being used in production as possible, or the benchmark isn't necessarily going to be accurate. And, in order to make benchmarks accurate, your machine also must be idle. I am doing this on my Mac, this machine is really technically not idle, but if you're gonna be running long-term benchmarks, especially on your desktop machines like this, make sure you're not browsing the web, or watching cat videos online, you really gotta leave the machine alone, because anything this machine's doing other than running the benchmark is effecting your performance. Now, another thing you have to understand is when we run these benchmarks, the Go compiler is going to recompile this code. The compiler has the ability to throw dead code away. In other words, the compiler could identify that this function doesn't mutate anything, yet it returns a value and we're not storing it. That means the compiler could say, I'm not gonna waste my time calling this function, because it has no impact on the output of this program whatsoever. So, notice that I'm performing the assignment to a local variable, and then to a global variable, so the compiler has no chance to throw this code away. You don't want this blazing fast benchmark, and you don't want it to be blazing fast because it didn't do anything. So, by default a benchmark will run for one second, which means it will run for a few seconds, and b.N will increase over time. So the first time it gets called, b.N is one, the next time, it will be larger, the next time, it will be larger, the idea is to run the code inside the loop enough times, hopefully millions of times, that we feel confident with the results. So, we've got our three benchmarks, link list, column, and row, and what I want to do right now is run these benchmarks. Now, I'm gonna run them a few times, because again, my machine is not idle, and I wanna see what the consistency is on every one of these runs. So I'm moving into my terminal window, I'm already in this location here, I'm already in the location where these files are, what I'm gonna do is ask the Go test tool to run the benchmarks. Now by default, the Go test tool wants to run test functions, these are functions that start with the word test, not benchmark, and the -run flag says, what test functions do you want me to filter out and only run. I'm saying none, it's any sort of regular expression you want, I'm saying none, because there are no test functions, and it can just very quickly forget about it. But I do want to run all the benchmarks, so I'm gonna use the . operator as the regular expression for all, and say run all the benchmarks. I'm also gonna increase the benchtime to three seconds from one, that way it just runs a little bit longer. Okay, let's do this CPU profile then, and we'll see what the benchmarks show us. So, there's our four million elements, or nodes, in our link list, we're starting with the link list, and every six digits would represent, at this point, a millisecond, so you can see there we're at 6.0 milliseconds for the traversal of the link list. You can see here we're at 10.4 milliseconds for the column traversal, and 3.6 milliseconds for the row traversal. Now this is already a really fascinating result. Remember that row and column traversal are using the same memory block, this is the same memory, not that it's allocating or working against something different, like the link list, it is the same block of memory. But look at the difference in performance, it's more than double the cost when you go column traversal over row, and the link list is sitting somewhere in between. I'm gonna run it again, because I like to see if there's any real consistency in the runs over time. So we're at 6.0 milliseconds, we're still at 6.0, it actually ran a little bit faster, 6.08 to 6.06, notice the drastic change in the column traversal, I mean, that's drastic, we're almost talking like three milliseconds, from our perspective, that's like eternity. And then the row traversal got a little faster, but again, it's very very consistent. Our link list and our row traversals are consistent, and our column traversal is all over the place. Alright, when I ran this for the first time, my head was spinning, I saw very, very similar results, and I'm saying to myself, what is going on here? Why is it that my algorithms are as efficient as they can be, yet the performance difference is in some cases, like column row, drastic, and in column and row, I'm using the same memory, it's not like I can explain it away that I'm somewhere else in memory, this is the same. Alright, so what I want to do is show you an animation that came from a great video from Scott Meyers where he talks about CPU caches and why they matter, and I really think everybody should watch this video. But here's an animation that Scott shows during the talk, and I want to walk through this with you, cause it's gonna help us to begin to explain these results that we see right here. Now, what this animation's gonna show us is the relative cache speed of accessing data from the different caches, you know that your processors have L1, L2, and L3 caches, and then there's main memory. So this is a level 1 transfer of memory, that's fast. Here's level 2, this is a level 3, a little bit slower, but there it is, and here's main memory, look at how slow it is. Main memory is so slow to access here, whew. Okay, it just finished, let's do this one more time, I want to show you again, the relative speed between these caches one more time. Level one, super fast, level two, a little slower, level three, still pretty slow, but we got it done, and then level four, oh my god, really, main memory, not level four. You know, I wish that I had something to read while we waited for this to get done, boom, there it is. That's the relative speed of fetching memory, accessing memory, from the different caches. Okay, how is that gonna help us understand what's going on? Well, let's take a look at this. This is the core i7-900 series, this processor is the one that Scott used when he put that animation together. And this processor is gonna allow us to talk about hardware at a representative level. We don't need to get into the guts and the deep, you know, understandings underneath the hardware, what we need to do is have a mechanical sympathy, an appreciation for how this hardware works, we can talk about hardware from a representative level, because when we have a piece of hardware like this processor that has a caching system, then from our perspective, everything is really the same in terms of what our responsibility is. So Scott used the i7-900, Intel i7-900 core series, and again, it's still very representative for the processors we use today. Now notice this processor, it's a four core processor, two hardware threads per core, and every core has its own L1 and L2 cache. Now, in this particular processor, the L1 is 64,000 bytes, split in half for instructions and data, and L2 is 256,000, L3 is eight meg, and then you have main memory. Now, one of the things that's interesting here, is that animation that I showed you is showing the relative clock, the clock cycle cost of accessing the data from these caches to get it into, let's say, that hardware thread, into the registers, so the hardware threads can do their job. That's going to be that latency cost that we saw in the animation. Now if I scroll down here just a little bit, we'll be able to start to see what those numbers were. Now, to best understand the latency in the numbers, let's start with this idea that the processor was a three gigahert clock, so now, if you notice, we have this three gigahert processor, but if you've really noticed lately, processors, not even lately, for years, processor speed has been going down, not up, a lot of that has to do with the fact that you have to deal with heat if you run these processors at a very high clock speed, but even though we've been bringing clock speeds down, you know, and the idea that clock speed is how we're pumping electrons through that processor, right, a three gigahert processor means that we have three clock cycles, or three pumps, every nanosecond, we've been bringing this down, but we've been getting faster and faster throughput in terms of instruction execution. Part of that is because we should be able, today, at least to execute about four instructions per clock cycle. And this is what we're really going after, we're not necessarily going after, can we burn the processors faster, can we raise the clock, can we execute more instructions per clock cycle, then we're gonna get better performance, and it's not unreasonable to get about four instructions per clock cycle, sometimes a little less, but we'd love to have maybe more. Which basically means that, we basically should be able to execute about 12 instructions per nanosecond on that three gigahert processor. Now, based on those numbers, that animation that I showed you was basically this, and you have to look at your hardware for the actual numbers for the hardware you're using, but on that i7-900 core series, to access data out of the L1 cache, that cost is four clock cycles of latency, or basically, for that core, we stalled for 16 instructions, or 1.3 nanoseconds. But that's the best we can do, right, I mean, that's gonna be our best latency in terms of accessing data. Now, if the data you need is in L2, well, on this hardware, that's costing us 11 clock cycles of latency, 44 instructions or 3.6 nanonseconds. Still, not so bad, right, you saw the animations, they weren't that bad, but again, a little bit slower. When data is in the L3 cache, well, now we just went from four, 11, to 39. Still within reason, but now we're looking at 156 instructions of stalling, or 13 nanoseconds. And finally, why main memory is so slow to access, is on this particular piece of hardware, it's costing us 107 clock cycles. This is like an order of magnitude larger, right, an order of magnitude more memory, but also an order of magnitude more slower. 107 clock cycles of latency, that's 428 instructions or 35 nanoseconds we've just stalled on that one core. And god forbid this is happening across all four. I mean, we could be talking about 1200 instructions that we are now not executing because the data we need is not in the caches, but in main memory. And main memory is just incredibly slow to access, this is why the caching systems are here, to help speed up this latency, to reduce the latency of getting data out of main memory and getting it there, close to where it's needed by those hardware threads. That is the thing, and most, the majority of the hardware you're gonna be using today, including your ARM processors and things, are gonna have these caching systems, so what we're talking about is going to be, you know, apply across all of the hardware and the systems that you're building when a caching system's involved. Alright, so this was the animation that we were looking at. We were looking at the relative clock speed of accessing data from the individual caches. And something else that's very interesting, at least on the Intel platforms, historically, that any data that's stored in L1 and L2 is copied or duplicated also in L3. So you can't add the three caching hierarchies together, from our perspective here, eight meg is the total amount of memory you have to work with if performance matters, and I want to make this clear, if performance matters, well, main memory is so slow to access, it might as well not even be there. If you've gotta keep taking a 107 clock cycle hit every time you need to access data, kiss your performance goodbye. So there's two core things here around the mechanical sympathy of the processor and the caching system, okay, main memory is so slow to access, it might as well not even be there, the total amount of memory you have is the total amount of cache, and in this case, eight meg. Also, small is fast, when we say small is fast, what we mean is, if the data you're working with is small enough to fit into the caches, and small enough to stay as close to the hardware thread as possible, you're also gonna see some better performance. Alright now let's talk about how these caching systems work, because we'll be able to tie that back into our results. Let's say on Core 0, we have an instruction that we want to execute, and that instruction needs the letter M, right here, in the word main memory, let's pretend that this is our data, and we need the letter M for main memory to come into the caches. Now it'd be very inefficient for the caching system to just pull in one byte into this L1, so what the hardware does, is it takes all of the memory that we work with, and it breaks it up into what we call cache lines, and a cache line, historically, is 64 bytes of memory, mainframes could have cache lines that are, like, twice that, 128, I also believe you can go into CMOS and make these changes, but 64 bytes is the historical size of a cache line. So you can think of all of the memory that's laid out on that machine, right, all of our virtual memory, and from a caching system perspective, it's the cache line. So the full cache line of memory that that letter M is on, so if this is holding the word Main, well, those letters are gonna be on some cache line, there's gonna be some more bytes here, more bytes here, the entire cache line is moved from main memory and into that L1 cache, now, when we're gonna move that cache line into the L1 cache, we're gonna have to move a cache line out. Remember, everything is sharing this, your operating system, code, other applications, we're all sharing the hardware, so we're gonna have to move some cache line out. The hardware's gotta figure out what cache line we should move, maybe it decides it wants to keep the cache line close, and move it into L2, which means we've gotta move a cache line out of L2 now, completely, and there is already a copy on these Intel processors of cache lines in L3, we get some mechanical sympathies with that, maybe we take a hit a little bit as data's coming in, because we're gonna have to put this cache line on L3, and maybe L1, and move something out of L1, into L2, this is why it takes so long, right, but once that's set, we're good to go, the AMD processors, historically, have not made those extra copies in L3, and you could add the three caching hierarchies together, and maybe get a little bit more memory to work with. I know the new Skylake processor from Intel is looking at having a much larger L2 cache than L3, and so some of those things would change. But, again, from our perspective, what's important is that we want to try to reduce that 107 clock cycle bit of latency where we're taking the cache line out of main, and moving it into L1, or say, L2, at the time we need it. What we would prefer to happen is that this cache line is already in L1 or L2 before we need it, because if we can get the cache lines into the processor before we need it, we don't take the 107 clock cycle, it's close, maybe we only take the four, or the 11 clock cycle hit, and we won't even feel that, it's all staying with inside the core. This is what we're doing, so the question then becomes, how can we create a situation where the cache line can be inside L1 or L2 before we need it? This is our job, this is something now that becomes our responsibility, what we have to do is write code that creates predictable access patterns to memory if we want to be mechanically sympathetic with the hardware. If performance matters, then what we have to do is be much more efficient with how data gets in to the processor, not get the processors to run at higher clock speed. Predictable access patterns to memory is everything. Alright, so how do you create a predictable access pattern to memory? That's the next step, here is the simplest way to do it. If you allocate a contiguous block of memory, and you walk through that memory on a predictable stride, well guess what, the prefetchers, which is little software sitting inside the processor, the prefetchers can pick up on that data access, and start bringing in those cache lines way ahead of when you need them. The prefetchers are everything, and we must be sympathetic with them. Now, the cleanest and easiest way to create this predictable access pattern is to use an array, an array gives us the ability to allocate a contiguous block of memory, and when you define an array, you define it based on an element size, right, it's an array of string, it's an array of int, there's our predictable stride, every index is a predictable stride from the other. Prefetchers love arrays, the hardware loves arrays, it really is the most important data structure there is today from the hardware. And I almost don't even care what your algorithms are, an array is going to beat it. Now there are times when maybe you're dealing with data that is so large that a linear traversal isn't going to be more efficient, but overall, if you're dealing with data, small data, that array and those predictable access patterns are going to beat out performance every time on these traversals. Alright, so the array is the most important data structure as it relates to the hardware, but I need to make it clear, the array is not the most important data structure in Go. The slice is the most important data structure in Go. And this isn't cause the slice uses an array underneath, technically, slice is really a vector, and if you've watched any C++ videos over the last five years, and go to CPP Khan and look, and if you see any CPP Khan video over the last five years of talking about performance, you will hear the person on stage say, vectors, vectors, vectors, vectors, vectors, use vectors. Why, because, just like the slice, we're gonna be using arrays behind the scenes, we're gonna be doing those linear iterations, and we're gonna be creating predictable access patterns to memory that the prefetchers are going to pick up on. Brilliant stuff. Now, there's another cache in the hardware called the TLB, this cache is also very important. When we talk about how the operating system manages memory, the operating system manages memory at a granularity called a page. Now, operating system pages can vary in size, anywhere from 4K, 8K, 16K, there's even some Linux distributions that use two meg pages, and this is why. The TLB is a very special cache that the operating system is gonna be managing. And what it does is it creates a cache of virtual addresses to operating system page and physical memory locations. In other words, your program is working in virtual memory, it thinks it's got real memory, but it's not, right, it's given a full virtual memory, because the operating system gives it that level of abstraction. So when you say, go get me the value at this address, this virtual address, the TLB is there so the hardware can do a very quick lookup, "Hey, where is this virtual address "for this process physically located in RAM?" That's the TLB, now, if you're getting TLB cache misses, and remember, these are caches, these are small caches, high performing caches, but they're small, and if you're getting a TLB miss, now what has to happen is the hardware's gotta go ask the operating system, "Where is this thing?" and there has to be, like, a traversal around the operating system paging table. And if you're on a virtual machine, like a cloud environment, you know, those virtual machines also have their own paging tables. You know, a TLB miss could be really, really deadly, and so these predictable access patterns are not just helping with cache lines, they're also helping with making sure the TLB is also properly cached, so all of this access to memory is as fast as possible. Again, performance today is not about how fast we can churn that clock, how fast we can push the clock in terms of cycles per nanosecond, it's about how efficiently we can get data into the processor before we need it, so we can reduce all of the latency there is in accessing data, especially in main, and if we can even reduce more of the L3 latency, well guess what, we're better off. So, if we go back to our results now, I want you to notice something here. We should be able to understand now why we see what see. Look at row traversal. Row traversal was not only the fastest, it was also incredibly consistent, and why is that? That is because, when we start walking through the matrix row by row, we're walking it down cache line by connected cache line. That row major traversal is creating a predictable access pattern to memory, and the prefetchers are coming in and reducing all of the main memory latency cost, it's brilliant, right, we're gonna get the best performance I can get on my machine through row major traversal. But why is column traversal so slow and inconsistent? I played a small game here with the matrix, I made the matrix large enough so this element and the next element not only were not really in a predictable stride, let's say, but made sure that those two elements ended up on two different operating system pages. Yes, they were very far away from each other, and on different operating system pages. I've got a ton of problems, this is basically pure access memory, random access memory, sorry, this is basically pure random memory access, when we're doing column traversal. This is why our results were so inconsistent and so slow. Now the link list sits somewhere in between. We're probably getting cache line misses, because this data is not guaranteed to be on a predictable stride, but we're probably getting this data all on the same page. This is why, when we talk about a two meg page, if you're dealing with a system, like a database, or something that's gonna be very, very large amounts of memory in data storage, those two meg pages can come in really, really handy, because the TLB means more data on a page, and your TLB cache will be better off for it. So we're probably getting cache line misses on the link list, but we're not getting so many TLB misses, and we're falling somewhere in between, really interesting. So we're starting to now understand why it's not so much how fast we push the clock, it's about how efficient we can get data into the processor, and efficiency's all going to be around creating predictable access patterns to memory. So now when we come back and we look at what Go's given us, arrays, slices, and maps, it all starts to make sense. We don't have link lists, and stacks, and queues, and these things, because they are not mechanically sympathetic with the hardware when your model is a real machine. I will never say anything negative about Java, the JVM is an amazing piece of engineering, because it takes these mechanical sympathy issues and deals with them for you, with compacting garbage collectors, and data layouts, they are making sure that when you have your linked list access, when your object orient patterns, which are all really link list oriented, that underneath the covers, without you realizing it, they're creating predictable access patterns for you. We don't have a virtual machine here. You have the real machine, which means you are more responsible for your data layouts, you're more responsible for your traversals, but that doesn't mean you have to get overwhelmed. What it means is, if you use the slice, which is the most important data structure in Go, not the array, but the slice, if you use it for the majority of your data needs, you are inherently creating predictable access patterns to memory, you are inherently being mechanically sympathetic, and by the way, the Go map also is constantly creating data underneath it that is contiguous. Its job is to keep the data contiguous. Think about it, your stacks, contiguous memory. Your maps, underneath, contiguous memory. Your slice, a vector, contiguous memory. Predictable access patterns are everything today, this is why that processor speed can go down, and we can still get some better performance, so this is what we have to understand, we want to use the slice, and a slice of values is our first choice, until it is not reasonable or practical to do so. And it may not be reasonable or practical because the problem you have in front of you may be easier to solve, simpler to solve, maybe, with a binary tree or a link list, and I'd rather you say to me, "Bill, I'm not using the slice, "because if I use this other data structure, "my life and the algorithms, everybody's life around us, "is going to be better, this is easier to maintain." And I'm going to say, "You know what, "that is worth the small performance hit we'll take." Cause you've already seen, over and over again, if you've been following from the beginning, that we take integrity, we take integrity first, right, which means we're gonna take some performance hits, we gotta, like zero value, we care about memory resources being low first, that's gonna take us some performance hits, so again, what we're trying to do here is maintain mechanical sympathies until it's not reasonable or practical to do so, and this is why Go's just given us the array, the slice, and the map.